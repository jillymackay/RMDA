{
  "hash": "9c280bc99f18ea94f9023ac214713a38",
  "result": {
    "markdown": "---\ntitle: \"Week 3: Introduction to Analyses\"\nformat: html\neditor: source\nexecute: \n  eval: FALSE\n  echo: TRUE\n  warning: FALSE\n  message: FALSE\n  freeze: TRUE\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n# Lecture 2: Introduction to statistics {.unnumbered}\n\n## Set up your environment and packages {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstan)\nlibrary(rstanarm)\n\ncat_weights <- tibble(avg_daily_snacks  = c(3, 2, 4, 2, 3, 1, 1, 0, 1, 0, 2, 3, 1, 2, 1, 3),\n                      weight = c(3.8, 3.9, 5, 3.7,  4.1, 3.6, 3.7, 3.6, 3.8, 4.1, 4.3, 3.9, 3.7, 3.8, 3.5, 4.3),\n                      environ = c(\"Indoor\", \"Indoor\", \"Outdoor\", \"Indoor\",\n                                  \"Outdoor\", \"Indoor\", \"Outdoor\", \"Indoor\",\n                                  \"Indoor\", \"Indoor\", \"Outdoor\", \"Indoor\",\n                                  \"Outdoor\", \"Indoor\", \"Indoor\", \"Outdoor\"))\n```\n:::\n\n\n\n\n### Summarise example data {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_weights |> \n  summarise(\"Mean Weight (kg)\" = mean(weight),\n            \"SD Weight (kg)\" = sd(weight),\n            \"Mean Daily Snacks\" = mean (avg_daily_snacks),\n            )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  `Mean Weight (kg)` `SD Weight (kg)` `Mean Daily Snacks`\n               <dbl>            <dbl>               <dbl>\n1               3.92            0.373                1.81\n```\n:::\n:::\n\n\n\n\n### Visualise example data {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat_weights |> \n  ggplot(aes(x = avg_daily_snacks, y = weight)) +\n  geom_point() +\n  labs(x = \"Average Daily Snacks\", y = \"Cat Weight\") +\n  theme_classic() +\n  scale_y_continuous(limits = c(0,5))\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### A Linear Model {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_fcat <- lm(weight ~ avg_daily_snacks, data = cat_weights)\nsummary(model_fcat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ avg_daily_snacks, data = cat_weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36758 -0.18723 -0.06116  0.06705  0.62813 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       3.55474    0.14045  25.309 4.33e-13 ***\navg_daily_snacks  0.20428    0.06576   3.107  0.00773 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2973 on 14 degrees of freedom\nMultiple R-squared:  0.4081,\tAdjusted R-squared:  0.3658 \nF-statistic: 9.652 on 1 and 14 DF,  p-value: 0.007729\n```\n:::\n\n```{.r .cell-code}\nreport::report(model_fcat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict weight with\navg_daily_snacks (formula: weight ~ avg_daily_snacks). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.41,\nF(1, 14) = 9.65, p = 0.008, adj. R2 = 0.37). The model's intercept,\ncorresponding to avg_daily_snacks = 0, is at 3.55 (95% CI [3.25, 3.86], t(14) =\n25.31, p < .001). Within this model:\n\n  - The effect of avg daily snacks is statistically significant and positive\n(beta = 0.20, 95% CI [0.06, 0.35], t(14) = 3.11, p = 0.008; Std. beta = 0.64,\n95% CI [0.20, 1.08])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n```\n:::\n\n```{.r .cell-code}\nparameters(model_fcat) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter        | Coefficient |   SE |       95% CI | t(14) |      p\n---------------------------------------------------------------------\n(Intercept)      |        3.55 | 0.14 | [3.25, 3.86] | 25.31 | < .001\navg daily snacks |        0.20 | 0.07 | [0.06, 0.35] |  3.11 | 0.008 \n```\n:::\n\n```{.r .cell-code}\nplot(model_parameters(model_fcat), show_intercept = TRUE)\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nplot(model_parameters(model_fcat))\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-4-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ncat_weights |> \n  ggplot(aes(x = avg_daily_snacks, y = weight)) +\n  geom_point() +\n  labs(x = \"Average Daily Snacks\", y = \"Cat Weight\",\n       caption = \"Weight ~ Average Daily Snacks shown\") +\n  theme_classic() +\n  scale_y_continuous(limits = c(0,5)) +\n  geom_abline(slope = 0.20, intercept = 3.55)\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-4-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### A Bayesian Model {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\n\nmodel_bcat <- stan_glm(weight ~ avg_daily_snacks, data = cat_weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.002048 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 20.48 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.053 seconds (Warm-up)\nChain 1:                0.04 seconds (Sampling)\nChain 1:                0.093 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.4e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 2:                0.042 seconds (Sampling)\nChain 2:                0.079 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 3:                0.038 seconds (Sampling)\nChain 3:                0.076 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 4:                0.039 seconds (Sampling)\nChain 4:                0.078 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nsummary(model_bcat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ avg_daily_snacks\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   2\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      3.6    0.2  3.4   3.6   3.7  \navg_daily_snacks 0.2    0.1  0.1   0.2   0.3  \nsigma            0.3    0.1  0.2   0.3   0.4  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 3.9    0.1  3.8   3.9   4.1  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3066 \navg_daily_snacks 0.0  1.0  2981 \nsigma            0.0  1.0  2539 \nmean_PPD         0.0  1.0  3573 \nlog-posterior    0.0  1.0  1450 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n:::\n\n```{.r .cell-code}\ndescribe_posterior(model_bcat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Posterior Distribution\n\nParameter        | Median |       95% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS\n-----------------------------------------------------------------------------------------------\n(Intercept)      |   3.55 | [3.26, 3.86] |   100% | [-0.04, 0.04] |        0% | 1.001 | 3066.00\navg_daily_snacks |   0.20 | [0.06, 0.35] | 99.62% | [-0.04, 0.04] |        0% | 1.000 | 2981.00\n```\n:::\n\n```{.r .cell-code}\nreport::report(model_bcat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 1:                0.051 seconds (Sampling)\nChain 1:                0.09 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 2:                0.047 seconds (Sampling)\nChain 2:                0.084 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 3:                0.05 seconds (Sampling)\nChain 3:                0.089 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 4:                0.058 seconds (Sampling)\nChain 4:                0.097 seconds (Total)\nChain 4: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a Bayesian linear model (estimated using MCMC sampling with 4 chains\nof 2000 iterations and a warmup of 1000) to predict weight with\navg_daily_snacks (formula: weight ~ avg_daily_snacks). Priors over parameters\nwere set as normal (mean = 0.00, SD = 0.80) distributions. The model's\nexplanatory power is substantial (R2 = 0.38, 95% CI [6.54e-06, 0.62], adj. R2 =\n0.15). The model's intercept, corresponding to avg_daily_snacks = 0, is at 3.55\n(95% CI [3.26, 3.86]). Within this model:\n\n  - The effect of avg daily snacks (Median = 0.20, 95% CI [0.06, 0.35]) has a\n99.62% probability of being positive (> 0), 99.30% of being significant (>\n0.02), and 90.10% of being large (> 0.11). The estimation successfully\nconverged (Rhat = 1.000) and the indices are reliable (ESS = 2981)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT)\nframework, we report the median of the posterior distribution and its 95% CI\n(Highest Density Interval), along the probability of direction (pd), the\nprobability of significance and the probability of being large. The thresholds\nbeyond which the effect is considered as significant (i.e., non-negligible) and\nlarge are |0.02| and |0.11| (corresponding respectively to 0.05 and 0.30 of the\noutcome's SD). Convergence and stability of the Bayesian sampling has been\nassessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and\nEffective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n```\n:::\n\n```{.r .cell-code}\nposteriors <- get_parameters(model_bcat)\n\nposteriors |> \n  ggplot(aes(x = avg_daily_snacks)) +\n  geom_density(fill = \"lightblue\") +\n  theme_classic() +\n  labs(x = \"Posterior Coefficient Estimates for Average Daily Snacks\",\n       y = \"Density\",\n       caption = \"Median Estimate Shown\") +\n  geom_vline(xintercept = 0.21, color = \"darkblue\", linewidth = 1)\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n### A Linear model with a factor {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_fcat2 <- lm(weight ~ avg_daily_snacks + environ, data = cat_weights)\nsummary(model_fcat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ avg_daily_snacks + environ, data = cat_weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2678 -0.1897 -0.0700  0.0821  0.5725 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       3.52748    0.13078  26.972 8.49e-13 ***\navg_daily_snacks  0.16168    0.06512   2.483   0.0275 *  \nenvironOutdoor    0.27860    0.15203   1.833   0.0899 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.275 on 13 degrees of freedom\nMultiple R-squared:  0.5296,\tAdjusted R-squared:  0.4572 \nF-statistic: 7.318 on 2 and 13 DF,  p-value: 0.007431\n```\n:::\n\n```{.r .cell-code}\nreport::report(model_fcat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict weight with\navg_daily_snacks and environ (formula: weight ~ avg_daily_snacks + environ).\nThe model explains a statistically significant and substantial proportion of\nvariance (R2 = 0.53, F(2, 13) = 7.32, p = 0.007, adj. R2 = 0.46). The model's\nintercept, corresponding to avg_daily_snacks = 0 and environ = Indoor, is at\n3.53 (95% CI [3.24, 3.81], t(13) = 26.97, p < .001). Within this model:\n\n  - The effect of avg daily snacks is statistically significant and positive\n(beta = 0.16, 95% CI [0.02, 0.30], t(13) = 2.48, p = 0.027; Std. beta = 0.51,\n95% CI [0.07, 0.95])\n  - The effect of environ [Outdoor] is statistically non-significant and positive\n(beta = 0.28, 95% CI [-0.05, 0.61], t(13) = 1.83, p = 0.090; Std. beta = 0.75,\n95% CI [-0.13, 1.63])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n```\n:::\n\n```{.r .cell-code}\nparameters(model_fcat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter         | Coefficient |   SE |        95% CI | t(13) |      p\n-----------------------------------------------------------------------\n(Intercept)       |        3.53 | 0.13 | [ 3.24, 3.81] | 26.97 | < .001\navg daily snacks  |        0.16 | 0.07 | [ 0.02, 0.30] |  2.48 | 0.027 \nenviron [Outdoor] |        0.28 | 0.15 | [-0.05, 0.61] |  1.83 | 0.090 \n```\n:::\n\n```{.r .cell-code}\nplot(model_parameters(model_fcat2), show_intercept = TRUE)\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nplot(model_parameters(model_fcat2))\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-6-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ncat_weights |> \n  ggplot(aes(x = avg_daily_snacks, y = weight, colour = environ)) +\n  geom_point() +\n  labs(x = \"Average Daily Snacks\", y = \"Cat Weight\",\n       caption = \"Weight ~ Average Daily Snacks shown\") +\n  theme_classic() +\n  scale_y_continuous(limits = c(0,5)) +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-6-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n### Bayesian Framework {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_bcat2 <- stan_glm(weight ~ avg_daily_snacks + environ, data = cat_weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.059 seconds (Warm-up)\nChain 1:                0.065 seconds (Sampling)\nChain 1:                0.124 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.052 seconds (Warm-up)\nChain 2:                0.047 seconds (Sampling)\nChain 2:                0.099 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.047 seconds (Warm-up)\nChain 3:                0.042 seconds (Sampling)\nChain 3:                0.089 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.058 seconds (Warm-up)\nChain 4:                0.046 seconds (Sampling)\nChain 4:                0.104 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nsummary(model_bcat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ avg_daily_snacks + environ\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   3\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      3.5    0.1  3.4   3.5   3.7  \navg_daily_snacks 0.2    0.1  0.1   0.2   0.2  \nenvironOutdoor   0.3    0.2  0.1   0.3   0.5  \nsigma            0.3    0.1  0.2   0.3   0.4  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 3.9    0.1  3.8   3.9   4.1  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3785 \navg_daily_snacks 0.0  1.0  3047 \nenvironOutdoor   0.0  1.0  2937 \nsigma            0.0  1.0  2680 \nmean_PPD         0.0  1.0  3807 \nlog-posterior    0.0  1.0  1463 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n:::\n\n```{.r .cell-code}\ndescribe_posterior(model_bcat2) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Posterior Distribution\n\nParameter        | Median |        95% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS\n------------------------------------------------------------------------------------------------\n(Intercept)      |   3.53 | [ 3.25, 3.81] |   100% | [-0.04, 0.04] |        0% | 1.000 | 3785.00\navg_daily_snacks |   0.16 | [ 0.02, 0.30] | 98.90% | [-0.04, 0.04] |     1.55% | 1.001 | 3047.00\nenvironOutdoor   |   0.28 | [-0.04, 0.62] | 95.65% | [-0.04, 0.04] |     4.18% | 1.000 | 2937.00\n```\n:::\n\n```{.r .cell-code}\nreport::report(model_bcat2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 1:                0.043 seconds (Sampling)\nChain 1:                0.1 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.062 seconds (Warm-up)\nChain 2:                0.071 seconds (Sampling)\nChain 2:                0.133 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.089 seconds (Warm-up)\nChain 3:                0.052 seconds (Sampling)\nChain 3:                0.141 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.073 seconds (Warm-up)\nChain 4:                0.098 seconds (Sampling)\nChain 4:                0.171 seconds (Total)\nChain 4: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a Bayesian linear model (estimated using MCMC sampling with 4 chains\nof 2000 iterations and a warmup of 1000) to predict weight with\navg_daily_snacks and environ (formula: weight ~ avg_daily_snacks + environ).\nPriors over parameters were all set as normal (mean = 0.00, SD = 0.80; mean =\n0.00, SD = 1.87) distributions. The model's explanatory power is substantial\n(R2 = 0.50, 95% CI [0.17, 0.74], adj. R2 = 0.26). The model's intercept,\ncorresponding to avg_daily_snacks = 0 and environ = Indoor, is at 3.53 (95% CI\n[3.25, 3.81]). Within this model:\n\n  - The effect of avg daily snacks (Median = 0.16, 95% CI [0.02, 0.30]) has a\n98.90% probability of being positive (> 0), 97.72% of being significant (>\n0.02), and 76.35% of being large (> 0.11). The estimation successfully\nconverged (Rhat = 1.001) and the indices are reliable (ESS = 3047)\n  - The effect of environ [Outdoor] (Median = 0.28, 95% CI [-0.04, 0.62]) has a\n95.65% probability of being positive (> 0), 94.53% of being significant (>\n0.02), and 85.28% of being large (> 0.11). The estimation successfully\nconverged (Rhat = 1.000) and the indices are reliable (ESS = 2937)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT)\nframework, we report the median of the posterior distribution and its 95% CI\n(Highest Density Interval), along the probability of direction (pd), the\nprobability of significance and the probability of being large. The thresholds\nbeyond which the effect is considered as significant (i.e., non-negligible) and\nlarge are |0.02| and |0.11| (corresponding respectively to 0.05 and 0.30 of the\noutcome's SD). Convergence and stability of the Bayesian sampling has been\nassessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and\nEffective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n```\n:::\n\n```{.r .cell-code}\nposteriors2 <- get_parameters(model_bcat2)\n\n\nposteriors2 |> \n  pivot_longer(cols = c(avg_daily_snacks, environOutdoor),\n               names_to = \"Parameter\",\n               values_to=\"estimate\") |> \n  ggplot() +\n  geom_density(aes(x = estimate, fill = Parameter)) +\n  theme_classic() +\n  labs(x = \"Posterior Coefficient Estimates\",\n       y = \"Density\") +\n  facet_wrap(facets = ~Parameter, ncol = 1) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n# Lecture: Calculating variance {.unnumbered}\n\n\n## Why does it matter? {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvardat <- tibble(cat = c(13, 17, 30, 36, 11, 43, 23, 50, 19, 23),\n                 dog = c(30, 31, 45, 43, 48, 50, 37, 32, 40, 44))\n\n\n\nvardat |> \n  pivot_longer(cols = c(cat, dog),\n               names_to = \"Species\",\n               values_to = \"Score\") |> \n  ggplot(aes(x = Species)) +\n  geom_point(aes(y = Score, colour = Species), position = position_jitter(width = .13), size = 1) +\n  see::geom_violinhalf(aes(y = Score, fill = Species), linetype = \"dashed\", position = position_nudge(x = .2)) +\n  geom_boxplot(aes(y = Score, alpha = 0.3, colour = Species), position = position_nudge(x = -.1), width = 0.1, outlier.shape = NA) +\n  theme_classic() +\n  labs(x = \"Species\", y = \"Score\") +\n  theme(legend.position = \"none\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Residuals {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid <- tibble(x = c(1, 2, 3),\n                y = c(3, 2, 6))\n\nresid |> \n  ggplot(aes(x, y)) +\n  geom_point(size = 4, colour = \"lightblue\") +\n  theme_classic() +\n  geom_hline(yintercept = 3.67) +\n  labs(title = \"Plot of 3 points, mean of y shown\") +\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.x = element_blank())\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-10-1.pdf)\n:::\n:::\n\n\n\n\n## Adding the residuals {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid |> \n  ggplot(aes(x, y)) +\n  geom_point(size = 4, colour = \"lightblue\") +\n  theme_classic() +\n  geom_hline(yintercept = 3.67) +\n  geom_segment(aes(x = 1, y = 3.67, xend = 1, yend = 3)) +\n  geom_segment(aes(x = 2, y = 3.67, xend = 2, yend = 2)) +\n  geom_segment(aes(x = 3, y = 3.67, xend = 3, yend = 6)) +\n  labs(title = \"Plot of 3 points, mean of y shown, residuals shown\") +\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.x = element_blank())\n```\n\n::: {.cell-output-display}\n![](week03_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Compare Variances {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvardat |> \n  summarise(var_dogs = var(dog),\n            var_cat = var(cat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  var_dogs var_cat\n     <dbl>   <dbl>\n1       52    169.\n```\n:::\n:::\n\n\n\n\n\n## Compare Standard Deviations {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvardat |> \n  summarise(sd_dogs = sd(dog),\n            sd_cat = sd(cat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  sd_dogs sd_cat\n    <dbl>  <dbl>\n1    7.21   13.0\n```\n:::\n:::\n\n\n\n\n\n## Compare Standard Errors {.unnumbered}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstd.error <- function(x) sd(x)/sqrt(length(x))\n\nvardat |> \n  summarise(se_dogs = std.error(dog),\n            se_cat = std.error(cat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  se_dogs se_cat\n    <dbl>  <dbl>\n1    2.28   4.11\n```\n:::\n:::\n\n\n\n\n\n\n#  Lecture: Meta Analyses {.unnumbered}\n\nCalculate rs from R2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(0.11)\n```\n:::\n",
    "supporting": [
      "week03_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}