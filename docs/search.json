[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods and Data Analysis (IAWEL)",
    "section": "",
    "text": "Preface\nThis book accompanies the Research Methods and Data Analysis course on the International Animal Welfare Ethics and Law MSc at the Royal (Dick) School of Veterinary Studies.\nIt is a companion document to the course, and not core to the materials.\nThroughout the RMDA Lectures, you will see a number of statistical tests, data visualisations, data manipulation, text mining, and simple calculations. Almost inevitably, each one of these steps will have been performed in R.\nYour R textbook is R@R(D)SVS, and that textbook will explain how to download and install R, how to run simple commands in R, and more. This RMDA textbook is like an accompanying document to your lecture materials, and is a place to help you move your R and statistical knowledge along."
  },
  {
    "objectID": "index.html#packages-in-this-textbook",
    "href": "index.html#packages-in-this-textbook",
    "title": "Research Methods and Data Analysis (IAWEL)",
    "section": "Packages in this textbook",
    "text": "Packages in this textbook\nThere are a range of packages used in this book, including Tidyverse (Wickham et al. 2019), effsize (Torchiano 2020), ggstatsplot (Patil 2021), vcd (Zeileis, Meyer, and Hornik 2007), wordcloud (Fellows 2018), easystats (Lüdecke et al. 2022), rstan (Stan Development Team 2023), rstanarm (Brilleman et al. 2018)\nYou may need to download and install a package or load a package for some of these commands to work."
  },
  {
    "objectID": "index.html#licensing",
    "href": "index.html#licensing",
    "title": "Research Methods and Data Analysis (IAWEL)",
    "section": "Licensing",
    "text": "Licensing\nThis book is licensed under the Unlicense.\nThis is free and unencumbered software released into the public domain.\nAnyone is free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means.\nIn jurisdictions that recognize copyright laws, the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nFor more information, please refer to https://unlicense.org\n\n\n\n\nBrilleman, SL, MJ Crowther, M Moreno-Betancur, J Buros Novik, and R Wolfe. 2018. “Joint Longitudinal and Time-to-Event Models via Stan.” https://github.com/stan-dev/stancon_talks/.\n\n\nFellows, Ian. 2018. Wordcloud: Word Clouds. https://CRAN.R-project.org/package=wordcloud.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Brenton M. Wiernik, and Dominique Makowski. 2022. “Easystats: Framework for Easy Statistical Modeling, Visualization, and Reporting.” CRAN. https://easystats.github.io/easystats/.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ’ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nStan Development Team. 2023. “RStan: The R Interface to Stan.” https://mc-stan.org/.\n\n\nTorchiano, Marco. 2020. Effsize: Efficient Effect Size Computation. https://doi.org/10.5281/zenodo.1480624.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856."
  },
  {
    "objectID": "week01.html",
    "href": "week01.html",
    "title": "2  Week 1: The Philosophy of Science",
    "section": "",
    "text": "Lecture 3: The Replication Crisis\nBland-Altmann Plots are generated with the following code.\nlibrary(tidyverse)\n\nbland &lt;-  tibble(\n  subject = c(1:17),\n  Wright1 = c(484,395,516,434,476,557,413,442,650,433,417,656,267,478,178,423,427),\n  Wright2 = c(490,397,512,401,470,611,415,431,638,429,420,633,275,492,165,372,421),\n  Mini1 = c(512,430,520,428,500,600,364,380,658,445,432,626,260,477,259,350,451),\n  Mini2 = c(525,415,508,444,500,625,460,390,642,432,420,605,227,467,268,370,443)\n)\n\nbland |&gt; \n  ggplot(aes(x = Wright1, y = Mini1)) +\n  stat_smooth(method=\"lm\", se=FALSE) +\n  geom_point(colour = \"turquoise4\", size = 3) +\n  scale_x_continuous(limits = c(0,700)) +\n  scale_y_continuous(limits = c(0,700)) +\n  theme_classic() +\n  labs(x = \"Wright Meter (1st Measure)\", y = \"Mini Wright Meter (1st Measure)\")\nAnd then if we add 100 to each measure, we see a very similar plot:\nbland |&gt; \n  mutate (Wright1 = (Wright1+100)) %&gt;%\n  ggplot(aes(x = Wright1, y = Mini1)) +\n  stat_smooth(method=\"lm\", se=FALSE) +\n  geom_point(colour = \"turquoise4\", size = 3) +\n  scale_x_continuous(limits = c(0,800)) +\n  scale_y_continuous(limits = c(0,700)) +\n  theme_classic() +\n  labs(x = \"Wright Meter (1st Measure)\", y = \"Mini Wright Meter (1st Measure)\")"
  },
  {
    "objectID": "week01.html#create-data-and-plot",
    "href": "week01.html#create-data-and-plot",
    "title": "2  Week 1: The Philosophy of Science",
    "section": "2.1 Create data and plot",
    "text": "2.1 Create data and plot\n\nlibrary(tidyverse)\n\nplants &lt;- tibble(none = c(4.8, 4.8, 3.94, 4.4,4.5,4.6),\n                 nutrients1  = c( 10.1, 9.7, 9.8, 9.9, 9.3, 10.1),\n                 nutrients2 = c(14.8, 14.6, 14.8, 14, 13.8, 14.6))\n\nplants |&gt; \n  pivot_longer(cols = c(none, nutrients1,nutrients2),\n               names_to = \"nutrients\",\n               values_to = \"height\") |&gt; \n  ggplot(aes(x = nutrients, y = height)) +\n  geom_boxplot()"
  },
  {
    "objectID": "week01.html#run-an-anova-on-plant-data",
    "href": "week01.html#run-an-anova-on-plant-data",
    "title": "2  Week 1: The Philosophy of Science",
    "section": "2.2 Run an ANOVA on Plant data",
    "text": "2.2 Run an ANOVA on Plant data\n\nlongplants &lt;- plants |&gt; \n  pivot_longer(cols = c(none, nutrients1,nutrients2),\n                             names_to = \"nutrients\",\n                             values_to = \"height\")\n\nplant_model &lt;- aov(height ~ nutrients, data = longplants)\n\nsummary(plant_model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nnutrients    2 296.10  148.05    1184 &lt;2e-16 ***\nResiduals   15   1.88    0.13                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "week01.html#read-and-run-crude-chicken-correlations",
    "href": "week01.html#read-and-run-crude-chicken-correlations",
    "title": "2  Week 1: The Philosophy of Science",
    "section": "2.3 Read and Run Crude Chicken Correlations",
    "text": "2.3 Read and Run Crude Chicken Correlations\n\ncrudechicks &lt;- tibble(year = c(\"2000\", \"2001\", \"2002\", \"2003\",\n                               \"2004\", \"2005\", \"2006\", \"2007\",\n                               \"2008\", \"2009\"),\n                      chicken = c(54.2, 54, 56.8, 57.5, 59.3, 60.5, 60.9,\n                                  59.9, 58.7, 56),\n                      crude = c(3311, 3405, 3336, 3521, 3674, 3670, 3685,\n                                3656, 3571, 3307))\n\n\ncor.test(crudechicks$chicken, crudechicks$crude, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  crudechicks$chicken and crudechicks$crude\nS = 20, p-value = 0.001977\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8787879"
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "",
    "text": "Lecture 2: Data Visualisation\nThis code will help you replicate the charts in Lecture 2"
  },
  {
    "objectID": "week02.html#height-vs-weight-by-sex",
    "href": "week02.html#height-vs-weight-by-sex",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Height vs Weight by Sex",
    "text": "Height vs Weight by Sex\n\nstarwars |&gt; \n  filter(species == \"Human\") |&gt; \n  ggplot(aes(x = height, y = mass, colour = sex)) +\n  geom_point() +\n  theme_classic() +\n  scale_x_continuous(limits = c(0,250)) +\n  scale_y_continuous(limits = c(0,150)) +\n  scale_colour_brewer(palette = \"Accent\", name = \"Sex\") +\n  theme(legend.position = \"bottom\") + \n  labs(x = \"Height (cm)\",\n       y = \"Weight (kg)\",\n       title = \"Height and Weight of Human Characters in Star Wars by Sex\")"
  },
  {
    "objectID": "week02.html#histogram-of-male-height",
    "href": "week02.html#histogram-of-male-height",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Histogram of male height",
    "text": "Histogram of male height\n\nstarwars |&gt; \n  filter(species == \"Human\",\n         sex == \"male\") |&gt;\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 3, fill = \"#bb9cd1\") +\n  theme_classic() +\n  labs(x = \"Height (cm)\",\n       y = \"Count\",\n       title = \"Histogram of height of male human Star Wars characters\")"
  },
  {
    "objectID": "week02.html#density-plot-of-male-height",
    "href": "week02.html#density-plot-of-male-height",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Density plot of male height",
    "text": "Density plot of male height\n\nstarwars |&gt; \n  filter(species == \"Human\",\n         sex == \"male\") |&gt;\n  ggplot(aes(x = height)) +\n  geom_density(fill = \"#bb9cd1\") +\n  theme_classic() +\n  labs(x = \"Height (cm)\",\n       y = \"Count\",\n       title = \"Density plot of height of male human Star Wars characters\")"
  },
  {
    "objectID": "week02.html#boxplot-of-height",
    "href": "week02.html#boxplot-of-height",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Boxplot of height",
    "text": "Boxplot of height\n\nstarwars |&gt; \n  filter(species == \"Human\") |&gt;\n  ggplot(aes(y = height, x = sex, colour = sex)) +\n  geom_boxplot() +\n  theme_classic() +\n  scale_colour_brewer(palette = \"Accent\", name = \"Sex\") +\n  labs(y = \"Height (cm)\",\n       x = \"Sex\",\n       title = \"Boxplot of height by sex of human characters in Star Wars\") +\n  theme(legend.position =\"none\")"
  },
  {
    "objectID": "week02.html#mean-height-bar-chart",
    "href": "week02.html#mean-height-bar-chart",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Mean height (bar chart)",
    "text": "Mean height (bar chart)\n\nstarwars |&gt; \n  filter(species == \"Human\") |&gt; \n  group_by(sex) |&gt; \n  summarise(ht = mean(height, na.rm = TRUE),\n            sd = sd(height, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = sex, y = ht, fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = ht-sd, ymax = ht+sd), width = 0.2)+\n  scale_fill_brewer(palette = \"Accent\") +\n  labs(y = \"Mean Height (cm)\",\n       x = \"Sex\",\n       title = \"Mean height by sex of human characters in Star Wars\") +\n  theme_classic() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "week02.html#mosaic-plot",
    "href": "week02.html#mosaic-plot",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Mosaic Plot",
    "text": "Mosaic Plot\n\nlibrary(vcd)\n\n\nstartbl &lt;-  starwars |&gt; \n  mutate(Species = fct_lump_n(species, 2),\n         EyeColour = fct_lump_n(eye_color,2)) \n\n\nmosaic(~ Species + EyeColour, data = startbl,shade = TRUE, legend = TRUE)"
  },
  {
    "objectID": "week02.html#pie-charts-are-just-bad-bar-charts",
    "href": "week02.html#pie-charts-are-just-bad-bar-charts",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Pie Charts are Just Bad Bar Charts",
    "text": "Pie Charts are Just Bad Bar Charts\n\nstarwars |&gt; \n  mutate(species = fct_lump_n(species, 4)) |&gt; \n  group_by(species) |&gt; \n  filter(!is.na(species)) |&gt; \n  tally() |&gt; \n  ggplot(aes(x = \"\", fill = species, y = n)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  theme_void() +\n  coord_polar(\"y\", start = 0)\n\n\n\nstarwars |&gt; \n  mutate(species = fct_lump_n(species, 4)) |&gt; \n  group_by(species) |&gt; \n  filter(!is.na(species)) |&gt; \n  tally() |&gt; \n  ggplot(aes(x = species, fill = species, y = n)) +\n  geom_bar(stat = \"identity\") +\n  theme_classic() +\n  labs(x = \"Species\", y = \"Count\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "week02.html#wordclouds",
    "href": "week02.html#wordclouds",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Wordclouds",
    "text": "Wordclouds\n\nlibrary(wordcloud)\nstarwars |&gt; \n  count(homeworld) |&gt; \n  with(wordcloud(words = homeworld, freq = n, min.freq=1, random.order = FALSE, rot.per = 0,\n                 colors = brewer.pal(6, \"Accent\"), use.r.layout = FALSE))"
  },
  {
    "objectID": "week02.html#raincloud-plots",
    "href": "week02.html#raincloud-plots",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Raincloud Plots",
    "text": "Raincloud Plots\n\nstarwars |&gt; \n  mutate(species = fct_lump_n(species,2)) |&gt; \n  filter(!is.na(species)) |&gt; \n  ggplot(aes(x = species)) +\n  geom_point(aes(y = height, colour = species), position = position_jitter(width = .13), size = 1, alpha = 0.6) +\n  see::geom_violinhalf(aes(y = height, alpha= 0.3, fill = species), linetype = \"dashed\", position = position_nudge(x = .2)) +\n  geom_boxplot(aes(y = height, alpha = 0.3, colour = species), position = position_nudge(x = -.1), width = 0.1, outlier.shape = NA) +\n  theme_classic() +\n  labs(x = \"Species\", y = \"Height (cm)\") +\n  theme(legend.position = \"none\") +\n  coord_flip()"
  },
  {
    "objectID": "week02.html#bubble-plots",
    "href": "week02.html#bubble-plots",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Bubble plots",
    "text": "Bubble plots\n\nstarwars |&gt;\n  mutate(col = fct_lump_n(species, 2)) |&gt; \n  ggplot(aes(x = birth_year, y = mass, size = height, colour = col)) +\n  geom_point() +\n  scale_size(range = c(.1, 24), name=\"Height\") +\n  theme_classic() +\n  scale_x_continuous(limits = c(0,250)) +\n  scale_y_continuous(limits = c(0,300)) +\n  scale_colour_brewer(palette = \"Accent\", name = \"Species\") +\n  theme(legend.position = \"bottom\") + \n  labs(x = \"Birth  Year (Before Battle of Yavin)\",\n       y = \"Weight (kg)\",\n       title = \"Weight by Age of Characters in Star Wars\")"
  },
  {
    "objectID": "week02.html#correlation-plots",
    "href": "week02.html#correlation-plots",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Correlation plots",
    "text": "Correlation plots\n\nstarwars |&gt;\n  select(height, mass, birth_year) |&gt; \n  ggcorrmat()"
  },
  {
    "objectID": "week02.html#data-and-custom-function-for-this-lecture",
    "href": "week02.html#data-and-custom-function-for-this-lecture",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Data and custom function for this lecture",
    "text": "Data and custom function for this lecture\n\nlibrary(tidyverse)\n\nheifers &lt;- tibble(heifers = c(211.3, 200.4, 220.1, 200.8, 222.0, 209.3, \n             195.8, 220.4, 226.2, 218.7, 193.7, 209.7))\n\n\n\nwage &lt;- readxl::read_excel(\"assets/UKWageData2023ONS.xlsx\", \n                           skip = 5)\n\nfind_mode &lt;- function(x) {\n  ux &lt;- unique(x)\n  tab &lt;- tabulate(match(x, ux))\n  ux[tab == max(tab)]\n}"
  },
  {
    "objectID": "week02.html#finding-central-tendency",
    "href": "week02.html#finding-central-tendency",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Finding central tendency",
    "text": "Finding central tendency\n\nheifers |&gt; \n  summarise(mean = mean(heifers),\n            median = median(heifers),\n            min = min(heifers),\n            max = max(heifers),\n            mode = find_mode(round(heifers, 0)))\n\n# A tibble: 1 × 5\n   mean median   min   max  mode\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  211.   210.  194.  226.   220\n\nwage |&gt; \n  summarise(mean = mean(Median),\n            median = median(Median),\n            min = min(Median),\n            max = max(Median),\n            mode = find_mode(round(Median,0)))\n\n# A tibble: 4 × 5\n    mean median   min   max  mode\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 34475.  31988 17859 84131 28216\n2 34475.  31988 17859 84131 35248\n3 34475.  31988 17859 84131 26000\n4 34475.  31988 17859 84131 25000"
  },
  {
    "objectID": "week02.html#the-mean-and-outliers",
    "href": "week02.html#the-mean-and-outliers",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "The Mean and Outliers",
    "text": "The Mean and Outliers\n\nheifers |&gt; \n  ggplot(aes(x = heifers)) +\n  geom_density(fill = \"#bb9cd1\") +\n  theme_classic() +\n  labs(x = \"Heifer Weight (kg)\",\n       y = \"Density\")\n\n\n\nheifers |&gt; \n  ggplot(aes(x = heifers)) +\n  geom_density(fill = \"#bb9cd1\") +\n  geom_vline(aes(xintercept = 210.7)) +\n  theme_classic() +\n  labs(x = \"Heifer Weight (kg)\",\n       y = \"Density\")"
  },
  {
    "objectID": "week02.html#mean-uk-salary",
    "href": "week02.html#mean-uk-salary",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Mean UK Salary",
    "text": "Mean UK Salary\n\nwage |&gt; \n  ggplot(aes(x = Median)) +\n  geom_density(fill = \"#bb9cd1\") +\n  theme_classic() +\n  geom_vline(aes(xintercept = 34475)) +\n  labs(x = \"UK Salaries (£)\",\n       y = \"Density\",\n       title = \"Distribution of UK Salaries\",\n       caption = \"Data taken from ONS 2023 Median Salaries by Field, n = 329 fields\")"
  },
  {
    "objectID": "week02.html#the-mode",
    "href": "week02.html#the-mode",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "The Mode",
    "text": "The Mode\n\nheifers |&gt; \n  ggplot(aes(x = heifers)) +\n  geom_histogram(fill = \"#bb9cd1\", binwidth = 1) +\n  geom_vline(aes(xintercept = 220)) +\n  theme_classic() +\n  labs(x = \"Heifer Weight (kg)\",\n       y = \"Density\")"
  },
  {
    "objectID": "week02.html#multiple-modes",
    "href": "week02.html#multiple-modes",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Multiple Modes",
    "text": "Multiple Modes\n\nwage |&gt; \n  ggplot(aes(x = Median)) +\n  geom_histogram(fill = \"#bb9cd1\", bins = 200) +\n  geom_vline(aes(xintercept = 25000)) +\n  geom_vline(aes(xintercept = 26000)) +\n  geom_vline(aes(xintercept = 28216)) +\n  geom_vline(aes(xintercept = 35248)) +\n  theme_classic() +\n  labs(x = \"UK Salaries (£)\",\n       y = \"Count\")"
  },
  {
    "objectID": "week02.html#the-median",
    "href": "week02.html#the-median",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "The Median",
    "text": "The Median\n\nheifers |&gt; \n  ggplot(aes(x = heifers)) +\n  geom_density(fill = \"#bb9cd1\") +\n  geom_vline(aes(xintercept = 210)) +\n  theme_classic() +\n  labs(x = \"Heifer Weight (kg)\",\n       y = \"Density\")"
  },
  {
    "objectID": "week02.html#median-uk-salary",
    "href": "week02.html#median-uk-salary",
    "title": "3  Week 2: The Use and Abuse of Data",
    "section": "Median UK Salary",
    "text": "Median UK Salary\n\nwage |&gt; \n  ggplot(aes(x = Median)) +\n  geom_density(fill = \"#bb9cd1\") +\n  theme_classic() +\n  geom_vline(aes(xintercept = 34475), colour = \"lightblue\") +\n  geom_vline(aes(xintercept = 31988),colour = \"purple\") +\n  labs(x = \"UK Salaries (£)\",\n       y = \"Density\",\n       title = \"Distribution of UK Salaries\",\n       caption = \"Data taken from ONS 2023 Median Salaries by Field, n = 329 fields\")"
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "",
    "text": "Lecture 2: Introduction to statistics\nCalculate rs from R2\nsqrt(0.11)"
  },
  {
    "objectID": "week03.html#set-up-your-environment-and-packages",
    "href": "week03.html#set-up-your-environment-and-packages",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "Set up your environment and packages",
    "text": "Set up your environment and packages\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstan)\nlibrary(rstanarm)\n\ncat_weights &lt;- tibble(avg_daily_snacks  = c(3, 2, 4, 2, 3, 1, 1, 0, 1, 0, 2, 3, 1, 2, 1, 3),\n                      weight = c(3.8, 3.9, 5, 3.7,  4.1, 3.6, 3.7, 3.6, 3.8, 4.1, 4.3, 3.9, 3.7, 3.8, 3.5, 4.3),\n                      environ = c(\"Indoor\", \"Indoor\", \"Outdoor\", \"Indoor\",\n                                  \"Outdoor\", \"Indoor\", \"Outdoor\", \"Indoor\",\n                                  \"Indoor\", \"Indoor\", \"Outdoor\", \"Indoor\",\n                                  \"Outdoor\", \"Indoor\", \"Indoor\", \"Outdoor\"))\n\n\nSummarise example data\n\ncat_weights |&gt; \n  summarise(\"Mean Weight (kg)\" = mean(weight),\n            \"SD Weight (kg)\" = sd(weight),\n            \"Mean Daily Snacks\" = mean (avg_daily_snacks),\n            )\n\n# A tibble: 1 × 3\n  `Mean Weight (kg)` `SD Weight (kg)` `Mean Daily Snacks`\n               &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;\n1               3.92            0.373                1.81\n\n\n\n\nVisualise example data\n\ncat_weights |&gt; \n  ggplot(aes(x = avg_daily_snacks, y = weight)) +\n  geom_point() +\n  labs(x = \"Average Daily Snacks\", y = \"Cat Weight\") +\n  theme_classic() +\n  scale_y_continuous(limits = c(0,5))\n\n\n\n\n\n\nA Linear Model\n\nmodel_fcat &lt;- lm(weight ~ avg_daily_snacks, data = cat_weights)\nsummary(model_fcat)\n\n\nCall:\nlm(formula = weight ~ avg_daily_snacks, data = cat_weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36758 -0.18723 -0.06116  0.06705  0.62813 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.55474    0.14045  25.309 4.33e-13 ***\navg_daily_snacks  0.20428    0.06576   3.107  0.00773 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2973 on 14 degrees of freedom\nMultiple R-squared:  0.4081,    Adjusted R-squared:  0.3658 \nF-statistic: 9.652 on 1 and 14 DF,  p-value: 0.007729\n\nreport::report(model_fcat)\n\nWe fitted a linear model (estimated using OLS) to predict weight with\navg_daily_snacks (formula: weight ~ avg_daily_snacks). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.41,\nF(1, 14) = 9.65, p = 0.008, adj. R2 = 0.37). The model's intercept,\ncorresponding to avg_daily_snacks = 0, is at 3.55 (95% CI [3.25, 3.86], t(14) =\n25.31, p &lt; .001). Within this model:\n\n  - The effect of avg daily snacks is statistically significant and positive\n(beta = 0.20, 95% CI [0.06, 0.35], t(14) = 3.11, p = 0.008; Std. beta = 0.64,\n95% CI [0.20, 1.08])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\nparameters(model_fcat) \n\nParameter        | Coefficient |   SE |       95% CI | t(14) |      p\n---------------------------------------------------------------------\n(Intercept)      |        3.55 | 0.14 | [3.25, 3.86] | 25.31 | &lt; .001\navg daily snacks |        0.20 | 0.07 | [0.06, 0.35] |  3.11 | 0.008 \n\nplot(model_parameters(model_fcat), show_intercept = TRUE)\n\n\n\nplot(model_parameters(model_fcat))\n\n\n\ncat_weights |&gt; \n  ggplot(aes(x = avg_daily_snacks, y = weight)) +\n  geom_point() +\n  labs(x = \"Average Daily Snacks\", y = \"Cat Weight\",\n       caption = \"Weight ~ Average Daily Snacks shown\") +\n  theme_classic() +\n  scale_y_continuous(limits = c(0,5)) +\n  geom_abline(slope = 0.20, intercept = 3.55)\n\n\n\n\n\n\nA Bayesian Model\n\nset.seed(10)\n\nmodel_bcat &lt;- stan_glm(weight ~ avg_daily_snacks, data = cat_weights)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.56 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.05 seconds (Warm-up)\nChain 1:                0.053 seconds (Sampling)\nChain 1:                0.103 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.4e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 2:                0.042 seconds (Sampling)\nChain 2:                0.079 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.3e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 3:                0.055 seconds (Sampling)\nChain 3:                0.101 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000578 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 5.78 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.054 seconds (Warm-up)\nChain 4:                0.063 seconds (Sampling)\nChain 4:                0.117 seconds (Total)\nChain 4: \n\nsummary(model_bcat)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ avg_daily_snacks\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   2\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      3.6    0.2  3.4   3.6   3.7  \navg_daily_snacks 0.2    0.1  0.1   0.2   0.3  \nsigma            0.3    0.1  0.2   0.3   0.4  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 3.9    0.1  3.8   3.9   4.1  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3066 \navg_daily_snacks 0.0  1.0  2981 \nsigma            0.0  1.0  2539 \nmean_PPD         0.0  1.0  3573 \nlog-posterior    0.0  1.0  1450 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\ndescribe_posterior(model_bcat)\n\nSummary of Posterior Distribution\n\nParameter        | Median |       95% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS\n-----------------------------------------------------------------------------------------------\n(Intercept)      |   3.55 | [3.26, 3.86] |   100% | [-0.04, 0.04] |        0% | 1.001 | 3066.00\navg_daily_snacks |   0.20 | [0.06, 0.35] | 99.62% | [-0.04, 0.04] |        0% | 1.000 | 2981.00\n\nreport::report(model_bcat)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.049 seconds (Warm-up)\nChain 1:                0.058 seconds (Sampling)\nChain 1:                0.107 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 2:                0.047 seconds (Sampling)\nChain 2:                0.084 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.037 seconds (Warm-up)\nChain 3:                0.047 seconds (Sampling)\nChain 3:                0.084 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 4:                0.055 seconds (Sampling)\nChain 4:                0.093 seconds (Total)\nChain 4: \n\n\nWe fitted a Bayesian linear model (estimated using MCMC sampling with 4 chains\nof 2000 iterations and a warmup of 1000) to predict weight with\navg_daily_snacks (formula: weight ~ avg_daily_snacks). Priors over parameters\nwere set as normal (mean = 0.00, SD = 0.80) distributions. The model's\nexplanatory power is substantial (R2 = 0.38, 95% CI [6.54e-06, 0.62], adj. R2 =\n0.15). The model's intercept, corresponding to avg_daily_snacks = 0, is at 3.55\n(95% CI [3.26, 3.86]). Within this model:\n\n  - The effect of avg daily snacks (Median = 0.20, 95% CI [0.06, 0.35]) has a\n99.62% probability of being positive (&gt; 0), 99.30% of being significant (&gt;\n0.02), and 90.10% of being large (&gt; 0.11). The estimation successfully\nconverged (Rhat = 1.000) and the indices are reliable (ESS = 2981)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT)\nframework, we report the median of the posterior distribution and its 95% CI\n(Highest Density Interval), along the probability of direction (pd), the\nprobability of significance and the probability of being large. The thresholds\nbeyond which the effect is considered as significant (i.e., non-negligible) and\nlarge are |0.02| and |0.11| (corresponding respectively to 0.05 and 0.30 of the\noutcome's SD). Convergence and stability of the Bayesian sampling has been\nassessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and\nEffective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\nposteriors &lt;- get_parameters(model_bcat)\n\nposteriors |&gt; \n  ggplot(aes(x = avg_daily_snacks)) +\n  geom_density(fill = \"lightblue\") +\n  theme_classic() +\n  labs(x = \"Posterior Coefficient Estimates for Average Daily Snacks\",\n       y = \"Density\",\n       caption = \"Median Estimate Shown\") +\n  geom_vline(xintercept = 0.21, color = \"darkblue\", linewidth = 1)\n\n\n\n\n\n\nA Linear model with a factor\n\nmodel_fcat2 &lt;- lm(weight ~ avg_daily_snacks + environ, data = cat_weights)\nsummary(model_fcat2)\n\n\nCall:\nlm(formula = weight ~ avg_daily_snacks + environ, data = cat_weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2678 -0.1897 -0.0700  0.0821  0.5725 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.52748    0.13078  26.972 8.49e-13 ***\navg_daily_snacks  0.16168    0.06512   2.483   0.0275 *  \nenvironOutdoor    0.27860    0.15203   1.833   0.0899 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.275 on 13 degrees of freedom\nMultiple R-squared:  0.5296,    Adjusted R-squared:  0.4572 \nF-statistic: 7.318 on 2 and 13 DF,  p-value: 0.007431\n\nreport::report(model_fcat2)\n\nWe fitted a linear model (estimated using OLS) to predict weight with\navg_daily_snacks and environ (formula: weight ~ avg_daily_snacks + environ).\nThe model explains a statistically significant and substantial proportion of\nvariance (R2 = 0.53, F(2, 13) = 7.32, p = 0.007, adj. R2 = 0.46). The model's\nintercept, corresponding to avg_daily_snacks = 0 and environ = Indoor, is at\n3.53 (95% CI [3.24, 3.81], t(13) = 26.97, p &lt; .001). Within this model:\n\n  - The effect of avg daily snacks is statistically significant and positive\n(beta = 0.16, 95% CI [0.02, 0.30], t(13) = 2.48, p = 0.027; Std. beta = 0.51,\n95% CI [0.07, 0.95])\n  - The effect of environ [Outdoor] is statistically non-significant and positive\n(beta = 0.28, 95% CI [-0.05, 0.61], t(13) = 1.83, p = 0.090; Std. beta = 0.75,\n95% CI [-0.13, 1.63])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\nparameters(model_fcat2)\n\nParameter         | Coefficient |   SE |        95% CI | t(13) |      p\n-----------------------------------------------------------------------\n(Intercept)       |        3.53 | 0.13 | [ 3.24, 3.81] | 26.97 | &lt; .001\navg daily snacks  |        0.16 | 0.07 | [ 0.02, 0.30] |  2.48 | 0.027 \nenviron [Outdoor] |        0.28 | 0.15 | [-0.05, 0.61] |  1.83 | 0.090 \n\nplot(model_parameters(model_fcat2), show_intercept = TRUE)\n\n\n\nplot(model_parameters(model_fcat2))\n\n\n\ncat_weights |&gt; \n  ggplot(aes(x = avg_daily_snacks, y = weight, colour = environ)) +\n  geom_point() +\n  labs(x = \"Average Daily Snacks\", y = \"Cat Weight\",\n       caption = \"Weight ~ Average Daily Snacks shown\") +\n  theme_classic() +\n  scale_y_continuous(limits = c(0,5)) +\n  geom_smooth()\n\n\n\n\n\n\nBayesian Framework\n\nmodel_bcat2 &lt;- stan_glm(weight ~ avg_daily_snacks + environ, data = cat_weights)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.063 seconds (Warm-up)\nChain 1:                0.068 seconds (Sampling)\nChain 1:                0.131 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.059 seconds (Warm-up)\nChain 2:                0.044 seconds (Sampling)\nChain 2:                0.103 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.058 seconds (Warm-up)\nChain 3:                0.053 seconds (Sampling)\nChain 3:                0.111 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.055 seconds (Warm-up)\nChain 4:                0.045 seconds (Sampling)\nChain 4:                0.1 seconds (Total)\nChain 4: \n\nsummary(model_bcat2)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ avg_daily_snacks + environ\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 16\n predictors:   3\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      3.5    0.1  3.4   3.5   3.7  \navg_daily_snacks 0.2    0.1  0.1   0.2   0.2  \nenvironOutdoor   0.3    0.2  0.1   0.3   0.5  \nsigma            0.3    0.1  0.2   0.3   0.4  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 3.9    0.1  3.8   3.9   4.1  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3785 \navg_daily_snacks 0.0  1.0  3047 \nenvironOutdoor   0.0  1.0  2937 \nsigma            0.0  1.0  2680 \nmean_PPD         0.0  1.0  3807 \nlog-posterior    0.0  1.0  1463 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\ndescribe_posterior(model_bcat2) \n\nSummary of Posterior Distribution\n\nParameter        | Median |        95% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS\n------------------------------------------------------------------------------------------------\n(Intercept)      |   3.53 | [ 3.25, 3.81] |   100% | [-0.04, 0.04] |        0% | 1.000 | 3785.00\navg_daily_snacks |   0.16 | [ 0.02, 0.30] | 98.90% | [-0.04, 0.04] |     1.55% | 1.001 | 3047.00\nenvironOutdoor   |   0.28 | [-0.04, 0.62] | 95.65% | [-0.04, 0.04] |     4.18% | 1.000 | 2937.00\n\nreport::report(model_bcat2)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.043 seconds (Warm-up)\nChain 1:                0.048 seconds (Sampling)\nChain 1:                0.091 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.048 seconds (Warm-up)\nChain 2:                0.039 seconds (Sampling)\nChain 2:                0.087 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.044 seconds (Warm-up)\nChain 3:                0.041 seconds (Sampling)\nChain 3:                0.085 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.048 seconds (Warm-up)\nChain 4:                0.075 seconds (Sampling)\nChain 4:                0.123 seconds (Total)\nChain 4: \n\n\nWe fitted a Bayesian linear model (estimated using MCMC sampling with 4 chains\nof 2000 iterations and a warmup of 1000) to predict weight with\navg_daily_snacks and environ (formula: weight ~ avg_daily_snacks + environ).\nPriors over parameters were all set as normal (mean = 0.00, SD = 0.80; mean =\n0.00, SD = 1.87) distributions. The model's explanatory power is substantial\n(R2 = 0.50, 95% CI [0.17, 0.74], adj. R2 = 0.26). The model's intercept,\ncorresponding to avg_daily_snacks = 0 and environ = Indoor, is at 3.53 (95% CI\n[3.25, 3.81]). Within this model:\n\n  - The effect of avg daily snacks (Median = 0.16, 95% CI [0.02, 0.30]) has a\n98.90% probability of being positive (&gt; 0), 97.72% of being significant (&gt;\n0.02), and 76.35% of being large (&gt; 0.11). The estimation successfully\nconverged (Rhat = 1.001) and the indices are reliable (ESS = 3047)\n  - The effect of environ [Outdoor] (Median = 0.28, 95% CI [-0.04, 0.62]) has a\n95.65% probability of being positive (&gt; 0), 94.53% of being significant (&gt;\n0.02), and 85.28% of being large (&gt; 0.11). The estimation successfully\nconverged (Rhat = 1.000) and the indices are reliable (ESS = 2937)\n\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT)\nframework, we report the median of the posterior distribution and its 95% CI\n(Highest Density Interval), along the probability of direction (pd), the\nprobability of significance and the probability of being large. The thresholds\nbeyond which the effect is considered as significant (i.e., non-negligible) and\nlarge are |0.02| and |0.11| (corresponding respectively to 0.05 and 0.30 of the\noutcome's SD). Convergence and stability of the Bayesian sampling has been\nassessed using R-hat, which should be below 1.01 (Vehtari et al., 2019), and\nEffective Sample Size (ESS), which should be greater than 1000 (Burkner, 2017).\n\nposteriors2 &lt;- get_parameters(model_bcat2)\n\n\nposteriors2 |&gt; \n  pivot_longer(cols = c(avg_daily_snacks, environOutdoor),\n               names_to = \"Parameter\",\n               values_to=\"estimate\") |&gt; \n  ggplot() +\n  geom_density(aes(x = estimate, fill = Parameter)) +\n  theme_classic() +\n  labs(x = \"Posterior Coefficient Estimates\",\n       y = \"Density\") +\n  facet_wrap(facets = ~Parameter, ncol = 1) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "week03.html#why-does-it-matter",
    "href": "week03.html#why-does-it-matter",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "Why does it matter?",
    "text": "Why does it matter?\n\nvardat &lt;- tibble(cat = c(13, 17, 30, 36, 11, 43, 23, 50, 19, 23),\n                 dog = c(30, 31, 45, 43, 48, 50, 37, 32, 40, 44))\n\n\n\nvardat |&gt; \n  pivot_longer(cols = c(cat, dog),\n               names_to = \"Species\",\n               values_to = \"Score\") |&gt; \n  ggplot(aes(x = Species)) +\n  geom_point(aes(y = Score, colour = Species), position = position_jitter(width = .13), size = 1) +\n  see::geom_violinhalf(aes(y = Score, fill = Species), linetype = \"dashed\", position = position_nudge(x = .2)) +\n  geom_boxplot(aes(y = Score, alpha = 0.3, colour = Species), position = position_nudge(x = -.1), width = 0.1, outlier.shape = NA) +\n  theme_classic() +\n  labs(x = \"Species\", y = \"Score\") +\n  theme(legend.position = \"none\") +\n  coord_flip()"
  },
  {
    "objectID": "week03.html#residuals",
    "href": "week03.html#residuals",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "Residuals",
    "text": "Residuals\n\nresid &lt;- tibble(x = c(1, 2, 3),\n                y = c(3, 2, 6))\n\nresid |&gt; \n  ggplot(aes(x, y)) +\n  geom_point(size = 4, colour = \"lightblue\") +\n  theme_classic() +\n  geom_hline(yintercept = 3.67) +\n  labs(title = \"Plot of 3 points, mean of y shown\") +\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.x = element_blank())"
  },
  {
    "objectID": "week03.html#adding-the-residuals",
    "href": "week03.html#adding-the-residuals",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "Adding the residuals",
    "text": "Adding the residuals\n\nresid |&gt; \n  ggplot(aes(x, y)) +\n  geom_point(size = 4, colour = \"lightblue\") +\n  theme_classic() +\n  geom_hline(yintercept = 3.67) +\n  geom_segment(aes(x = 1, y = 3.67, xend = 1, yend = 3)) +\n  geom_segment(aes(x = 2, y = 3.67, xend = 2, yend = 2)) +\n  geom_segment(aes(x = 3, y = 3.67, xend = 3, yend = 6)) +\n  labs(title = \"Plot of 3 points, mean of y shown, residuals shown\") +\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.x = element_blank())"
  },
  {
    "objectID": "week03.html#compare-variances",
    "href": "week03.html#compare-variances",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "Compare Variances",
    "text": "Compare Variances\n\nvardat |&gt; \n  summarise(var_dogs = var(dog),\n            var_cat = var(cat))\n\n# A tibble: 1 × 2\n  var_dogs var_cat\n     &lt;dbl&gt;   &lt;dbl&gt;\n1       52    169."
  },
  {
    "objectID": "week03.html#compare-standard-deviations",
    "href": "week03.html#compare-standard-deviations",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "Compare Standard Deviations",
    "text": "Compare Standard Deviations\n\nvardat |&gt; \n  summarise(sd_dogs = sd(dog),\n            sd_cat = sd(cat))\n\n# A tibble: 1 × 2\n  sd_dogs sd_cat\n    &lt;dbl&gt;  &lt;dbl&gt;\n1    7.21   13.0"
  },
  {
    "objectID": "week03.html#compare-standard-errors",
    "href": "week03.html#compare-standard-errors",
    "title": "4  Week 3: Introduction to Analyses",
    "section": "Compare Standard Errors",
    "text": "Compare Standard Errors\n\nstd.error &lt;- function(x) sd(x)/sqrt(length(x))\n\nvardat |&gt; \n  summarise(se_dogs = std.error(dog),\n            se_cat = std.error(cat))\n\n# A tibble: 1 × 2\n  se_dogs se_cat\n    &lt;dbl&gt;  &lt;dbl&gt;\n1    2.28   4.11"
  },
  {
    "objectID": "week04.html",
    "href": "week04.html",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "",
    "text": "Lecture: Effect Sizes and Covariance"
  },
  {
    "objectID": "week04.html#mock-data-and-visualisation",
    "href": "week04.html#mock-data-and-visualisation",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "Mock Data and visualisation",
    "text": "Mock Data and visualisation\n\njob_dat &lt;- tibble(job = c(\"vet\", \"vet\", \"vet\",\"vet\", \"vet\", \"vet\", \"vet\", \"vet\", \"vet\", \"vet\",\n                          \"assc\", \"assc\", \"assc\", \"assc\", \"assc\", \"assc\", \"assc\", \"assc\", \"assc\", \"assc\"),\n                  burnout = c(13, 12, 4, 16, 16, 20, 8, 10, 11, 10,\n                              10, 11, 8, 7, 8, 10, 9, 11, 17, 10),\n                  empathy = c(4, 5, 1, 4,3, 5, 2, 3,3,2,\n                              2, 3, 3, 2, 2, 3, 3, 4, 5, 2),\n                  satisfaction = c(\"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\",\n                                   \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\",\"no\", \"yes\", \"yes\"))\n\n\njob_dat |&gt; \n  ggplot(aes(x = burnout, y = empathy, shape = job, colour = satisfaction)) +\n  geom_point() +\n  theme_classic() +\n  labs(title = \"Burnout and empathy scores for vets and associated professions\",\n       subtitle = \"Job Satisfaction shown\",\n       caption = \"Mock data for teaching\",\n       x = \"Burnout Score\",\n       y = \"Empathy Score\") +\n  scale_shape_discrete(name = \"Vet or Associated Profession\") +\n  scale_color_discrete(name = \"Satisfied with job?\")"
  },
  {
    "objectID": "week04.html#calculcate-cohens-d",
    "href": "week04.html#calculcate-cohens-d",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "Calculcate Cohen’s d",
    "text": "Calculcate Cohen’s d\n\nlibrary(effsize)\n\ncohen.d(d = job_dat$burnout, f = job_dat$job)\n\n\nCohen's d\n\nd estimate: -0.5048995 (medium)\n95 percent confidence interval:\n     lower      upper \n-1.4593128  0.4495138"
  },
  {
    "objectID": "week04.html#calculcate-hedges-g",
    "href": "week04.html#calculcate-hedges-g",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "Calculcate Hedge’s g",
    "text": "Calculcate Hedge’s g\n\ncohen.d(d = job_dat$burnout, f = job_dat$job, hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: -0.4835657 (small)\n95 percent confidence interval:\n     lower      upper \n-1.3964834  0.4293519"
  },
  {
    "objectID": "week04.html#effects-of-differences",
    "href": "week04.html#effects-of-differences",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "Effects of Differences",
    "text": "Effects of Differences\n\njob_dat |&gt; \n  ggplot(aes(x = burnout, y = empathy)) +\n  geom_point() +\n  theme_classic() +\n  labs(x = \"Burnout Score\", y = \"Empathy Score\")"
  },
  {
    "objectID": "week04.html#correlation-coefficient-r",
    "href": "week04.html#correlation-coefficient-r",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "Correlation coefficient (r)",
    "text": "Correlation coefficient (r)\n\ncor(job_dat$burnout, job_dat$empathy, method = \"pearson\")\n\n[1] 0.7991678"
  },
  {
    "objectID": "week04.html#other-correlation-coefficients",
    "href": "week04.html#other-correlation-coefficients",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "Other Correlation Coefficients",
    "text": "Other Correlation Coefficients\n\ncor(job_dat$burnout, job_dat$empathy, method = \"spearman\")\n\n[1] 0.8202187\n\ncor.test(job_dat$burnout, as.numeric(as.factor(job_dat$job)))\n\n\n    Pearson's product-moment correlation\n\ndata:  job_dat$burnout and as.numeric(as.factor(job_dat$job))\nt = 1.129, df = 18, p-value = 0.2737\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2091671  0.6281908\nsample estimates:\n      cor \n0.2571562"
  },
  {
    "objectID": "week04.html#cramers-v",
    "href": "week04.html#cramers-v",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "Cramer’s V",
    "text": "Cramer’s V\n\nlibrary(lsr)\n\njob_dat &lt;- job_dat |&gt; \n  mutate(burnoutcat = case_when(burnout &gt; 10 ~ \"burnout\",\n                                TRUE ~ \"no burnout\"))\n\n\njob_tbl &lt;- xtabs(~job_dat$job +  job_dat$satisfaction + job_dat$burnoutcat)\nftable(job_tbl)\n\n                                 job_dat$burnoutcat burnout no burnout\njob_dat$job job_dat$satisfaction                                      \nassc        no                                            1          1\n            yes                                           2          6\nvet         no                                            3          2\n            yes                                           3          2\n\nchisq.test(ftable(job_tbl))\n\n\n    Pearson's Chi-squared test\n\ndata:  ftable(job_tbl)\nX-squared = 2.2222, df = 3, p-value = 0.5276\n\ncramersV(ftable(job_tbl))\n\n[1] 0.3333333"
  },
  {
    "objectID": "week04.html#r2adj-example",
    "href": "week04.html#r2adj-example",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "R2adj Example",
    "text": "R2adj Example\n\njobmod &lt;- lm(burnout ~ empathy + satisfaction, data = job_dat)\nsummary(jobmod)\n\n\nCall:\nlm(formula = burnout ~ empathy + satisfaction, data = job_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7526 -1.0832 -0.3836  1.4822  4.7305 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.9944     1.8904   1.055    0.306    \nempathy           2.7516     0.4865   5.656 2.85e-05 ***\nsatisfactionyes   1.0202     1.1393   0.895    0.383    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.354 on 17 degrees of freedom\nMultiple R-squared:  0.6549,    Adjusted R-squared:  0.6144 \nF-statistic: 16.13 on 2 and 17 DF,  p-value: 0.000118\n\njob_dat |&gt; \n  mutate(mod = predict(jobmod)) |&gt; \n  ggplot() + \n  geom_point(aes(x = empathy, y = burnout, colour = satisfaction)) +\n  geom_line(aes(x = empathy, y = mod)) +\n  theme_classic() +\n  facet_wrap(facets = ~ satisfaction, ncol = 1) +\n  labs(x = \"Empathy Score\", y = \"Burnout Score\")"
  },
  {
    "objectID": "week04.html#covariance-.-unnumbered",
    "href": "week04.html#covariance-.-unnumbered",
    "title": "5  Week 4: Considerations for Collecting Data",
    "section": "5.1 Covariance {. unnumbered}",
    "text": "5.1 Covariance {. unnumbered}\n\njobmod2 &lt;- lm(burnout ~ empathy + job, data = job_dat)\nsummary(jobmod2)\n\n\nCall:\nlm(formula = burnout ~ empathy + job, data = job_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6359 -1.3894 -0.4212  1.6035  4.5151 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.631      1.552   1.695    0.108    \nempathy        2.575      0.471   5.469 4.16e-05 ***\njobvet         1.127      1.052   1.072    0.299    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.331 on 17 degrees of freedom\nMultiple R-squared:  0.6615,    Adjusted R-squared:  0.6217 \nF-statistic: 16.61 on 2 and 17 DF,  p-value: 0.0001002\n\njob_dat |&gt; \n  mutate(mod = predict(jobmod2)) |&gt; \n  ggplot() + \n  geom_point(aes(x = empathy, y = burnout, colour = job)) +\n  geom_line(aes(x = empathy, y = mod)) +\n  theme_classic() +\n  facet_wrap(facets = ~ job, ncol = 1) +\n  labs(x = \"Empathy Score\", y = \"Burnout Score\")"
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "6  Week 5: Sources of Data",
    "section": "",
    "text": "Lecture 1 Digital Media Research"
  },
  {
    "objectID": "week05.html#whos-in-digital-spaces",
    "href": "week05.html#whos-in-digital-spaces",
    "title": "6  Week 5: Sources of Data",
    "section": "6.1 Who’s in digital spaces",
    "text": "6.1 Who’s in digital spaces\n\nlibrary(tidyverse)\n\ninternet &lt;- tibble(year = c(2013, 2014, 2015, 2016, 2017,   2018,   2019,   2020),\n                   \"16-24\" = c(98.3,    98.9,   98.8,   99.2,   99.2,   99.3,   99.2,   99.5),\n                   \"25-34\" = c(97.7,    98.3,   98.6,   98.9,   99.1,   99.2,   99.4,   99.5),\n                   \"35-44\" = c(95.8,    96.7,   97.3,   98.2,   98.4,   98.6,   98.9,   99.1),\n                   \"45-54\" = c(90.2,    92.3,   93.6,   94.9,   96.2,   96.8,   97.5,   97.9),\n                   \"55-64\" = c(81.3,    84.2,   86.7,   88.3,   90.0,   91.8,   93.2,   94.6),\n                   \"65-74\" = c(61.1,    65.5,   70.6,   74.1,   77.5,   80.2,   83.2,   85.5),\n                  \"75+\" = c(29.1,   31.9,   33.0,   38.7,   40.5,   43.6,   46.8,   54.0)) |&gt; \n  pivot_longer(cols = -year,\n               names_to = \"age\",\n               values_to = \"perc\")\n\n\ninternet |&gt;\n  ggplot(aes(x = year, y = perc, fill = age)) +\n  geom_bar(stat = \"identity\", position = \"dodge2\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Year\", y = \"Percentage of Adults\",\n       caption = \"UK adults who used the internet in the last 3 months (Jan-March)\\nData from ONS\")"
  },
  {
    "objectID": "week06.html",
    "href": "week06.html",
    "title": "7  Week 6: Analysing Qualitative Data",
    "section": "",
    "text": "There’s no content in this week this year :)"
  },
  {
    "objectID": "week07.html",
    "href": "week07.html",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "",
    "text": "Lecture 1: Partitioning Variation\nlibrary(tidyverse)\n\n\nexamples &lt;- tibble (x.perfect = c(1,2,3,4,5,6,7,8,9,10), \n                    y.perfect = c(1,2,3,4,5,6,7,8,9,10), \n                    x.realistic = c(1,2,3,4,4,6,8,8,9,11), \n                    y.realistic = c(2, 2, 4, 5, 5, 5, 7, 8, 9, 9))\nlibrary(tidyverse)\n\ncows &lt;- readxl::read_excel(\"cows.xlsx\") |&gt; \n  mutate(parlour = as.factor(parlour),\n         `housing type` = as.factor(`housing type`))\n\n\ncows |&gt; \n  ggplot(aes(x = `Welfare Score`, y= `Average Daily Yield`, colour = `housing type`)) +\n  geom_point() +\n  theme_classic() +\n  labs(x  = \"Welfare Score\", y = \"Average Daily Yield\",\n       title = \"Milk Yield versus Welfare Score\",\n       subtitle = \"For Robotic vs Manual Parlours\") +\n  facet_wrap(facets = ~parlour, ncol = 1) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "week07.html#a-perfect-world",
    "href": "week07.html#a-perfect-world",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "A Perfect World",
    "text": "A Perfect World\n\nexamples |&gt; \n  ggplot (aes(x = x.perfect, y = y.perfect))+\n  geom_point (shape = 8) +\n  scale_y_continuous(limits =c(0,10)) +\n  scale_x_continuous(limits = c(0,10)) +\n  labs (title = \"100% of the variation in y is explained by x\",\n        x = \"A Perfect Explanatory Variable\",\n        y = \"A Perfect Response Variable\") +\n  theme_classic()"
  },
  {
    "objectID": "week07.html#our-unicorn-farm",
    "href": "week07.html#our-unicorn-farm",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Our Unicorn Farm",
    "text": "Our Unicorn Farm\n\nunicorns &lt;- tibble (NoRadio = c(150, 130, 121, 90, 98, 100, 98, 100, 113, 111),\n                 RadioMusic = c(112, 127, 132, 150, 112, 128, 110, 120, 98, 107),\n                 RadioDiscussion = c(75, 98, 88, 83, 83, 77, 75, 84, 93, 99)) |&gt; \n  pivot_longer(cols = c(NoRadio:RadioDiscussion),\n               names_to = \"Radio\",\n               values_to = \"DustYield\")\n\n\nunicorns |&gt; \n  ggplot (aes (x = Radio, y = DustYield)) + \n  geom_point(aes(shape = Radio, colour = Radio),  position = position_jitter(width = .13)) +\n  labs (y = \"Dust Yield (Kg)\", x = \"Radio Condition\") +\n  scale_color_brewer(palette = \"Accent\") +\n  scale_y_continuous(limits = c(0, 200)) +\n  theme_classic () +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "week07.html#no-radio-group-.unnumbered",
    "href": "week07.html#no-radio-group-.unnumbered",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "8.1 No Radio Group .{unnumbered}",
    "text": "8.1 No Radio Group .{unnumbered}\n\nunicorns |&gt;\n  filter(Radio == \"NoRadio\") |&gt; \n  mutate(UnicornNo = c(1,2,3,4,5,6,7,8,9,10)) |&gt; \n  # The mutate function adds a new variable just to plot this one specific chart\n  # And then we pipe it directly into ggplot, so we're not changing the unicorns data\n  # Remember you can check this with `View(unicorns)`, you'll see 'UnicornNo' doesn't exist.\n  ggplot (aes (x = UnicornNo, y = DustYield)) + \n  geom_point() +\n  labs (y = \"Dust Yield (Kg)\", x = \"Unicorn No\") +\n  scale_y_continuous(limits = c(0, 200)) +\n  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) + \n  theme_classic ()"
  },
  {
    "objectID": "week07.html#no-radio-mean-.unnumbered",
    "href": "week07.html#no-radio-mean-.unnumbered",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "8.2 No Radio Mean .{unnumbered}",
    "text": "8.2 No Radio Mean .{unnumbered}\n\nunicorns |&gt;\n  group_by(Radio) |&gt;\n  filter(Radio == \"NoRadio\") |&gt;\n  summarise(mean = mean(DustYield))\n\n# A tibble: 1 × 2\n  Radio    mean\n  &lt;chr&gt;   &lt;dbl&gt;\n1 NoRadio  111.\n\nunicorns |&gt;\n  filter(Radio == \"NoRadio\") |&gt; \n  mutate(UnicornNo = c(1,2,3,4,5,6,7,8,9,10)) |&gt; \n  # The mutate function adds a new variable just to plot this one specific chart\n  # And then we pipe it directly into ggplot, so we're not changing the unicorns data\n  # Remember you can check this with `View(unicorns)`, you'll see 'UnicornNo' doesn't exist.\n  ggplot (aes (x = UnicornNo, y = DustYield)) + \n  geom_point() +\n  geom_hline(yintercept = 111.1)+\n  labs (y = \"Dust Yield (Kg)\", x = \"Unicorn No\") +\n  scale_y_continuous(limits = c(0, 200)) +\n  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) + \n  theme_classic ()"
  },
  {
    "objectID": "week07.html#deviations-from-the-mean-.unnumbered",
    "href": "week07.html#deviations-from-the-mean-.unnumbered",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "8.3 Deviations from the mean .{unnumbered}",
    "text": "8.3 Deviations from the mean .{unnumbered}\n\nunicorns |&gt; \n  filter(Radio == \"NoRadio\") |&gt; \n  mutate(Diff = DustYield-111.1)\n\n# A tibble: 10 × 3\n   Radio   DustYield    Diff\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 NoRadio       150  38.9  \n 2 NoRadio       130  18.9  \n 3 NoRadio       121   9.9  \n 4 NoRadio        90 -21.1  \n 5 NoRadio        98 -13.1  \n 6 NoRadio       100 -11.1  \n 7 NoRadio        98 -13.1  \n 8 NoRadio       100 -11.1  \n 9 NoRadio       113   1.90 \n10 NoRadio       111  -0.100\n\nunicorns |&gt; \n  filter(Radio == \"NoRadio\") |&gt; \n  mutate(Diff = DustYield-111.1) |&gt; \n  summarise(`Sum of Differences` = sum(Diff))\n\n# A tibble: 1 × 1\n  `Sum of Differences`\n                 &lt;dbl&gt;\n1             5.68e-14"
  },
  {
    "objectID": "week07.html#variance-.unnumbered",
    "href": "week07.html#variance-.unnumbered",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "8.4 Variance .{unnumbered}",
    "text": "8.4 Variance .{unnumbered}\n\n#: eval: true\n\n\nunicorns |&gt; \n  group_by(Radio) |&gt; \n  summarise (Variance = var(DustYield))"
  },
  {
    "objectID": "week07.html#imaginary-scenarions-.unnumbered",
    "href": "week07.html#imaginary-scenarions-.unnumbered",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "8.5 Imaginary Scenarions .{unnumbered}",
    "text": "8.5 Imaginary Scenarions .{unnumbered}\n\nSce1 &lt;- tibble (Condition1 = c(99,100,101,99,100,101,99,101,100,101),\n                Condition2 = c(120,121,122,120,121,122,120,123,121,120),\n                Condition3 = c(83,84,85,85,84,83,83,84,85,86)) |&gt;\n  pivot_longer (cols = c(Condition1:Condition3), names_to = \"Condition\", values_to = \"DustYield\") |&gt;\n  mutate (Count = c(1:30)) \n\nSce2 &lt;-tibble (Condition1 = c(84,86,123,95,87,110,99,95,121,121),\n               Condition2 = c(83,115,85,85, 110,105,84,115,101,100),\n               Condition3 = c(84,122,80,80,101,83,83,99, 120,120)) |&gt;\n  pivot_longer (cols = c(Condition1:Condition3), names_to = \"Condition\", values_to = \"DustYield\") |&gt;\n  mutate (Count = c(1:30)) \n\n\n\n\nImaginaryScenario1 &lt;- Sce1 |&gt;\n  ggplot (aes (x = Count, y = DustYield)) + \n  geom_point(aes(shape = Condition, colour = Condition)) +\n  labs (y = \"Dust Yield (Kg)\", x = \"Unicorn ID Number\") +\n  scale_y_continuous(limits = c(0, 200)) +\n  theme_classic ()\n\n# I have also made this chart an object because we're going to update it\n# It's quicker to do this as an object\n# You can compare how we update this chart with how we update the ones above.\n\nImaginaryScenario1\n\n\n\nexperiment &lt;- Sce2 |&gt; \n  ggplot (aes (x = Count, y = DustYield)) + \n  geom_point(aes(shape = Condition, colour = Condition)) +\n  labs (y = \"Dust Yield (Kg)\", x = \"Unicorn ID Number\") +\n  scale_y_continuous(limits = c(0, 200)) +\n  theme_classic ()\n\nexperiment\n\n\n\nImaginaryScenario1 +\n  geom_hline(yintercept = mean(Sce1$DustYield))\n\n\n\nexperiment +\n  geom_hline(yintercept = mean(Sce2$DustYield))\n\n\n\nImaginaryScenario1 +\n  geom_hline(yintercept = mean(Sce1$DustYield)) +\n  geom_segment(aes(x =1, y = 100.1, xend =30, yend = 100.1, color = \"red\")) +\n  geom_segment(aes(x = 1, y = 121.0, xend = 30, yend = 121.0, color = \"green\")) +\n  geom_segment (aes(x = 1, y = 84.2, xend = 30, yend = 84.2, color = \"blue\")) +\n  theme (legend.position = \"none\")\n\n\n\nexperiment +\n  geom_hline(yintercept = mean(Sce1$DustYield)) +\n  geom_segment(aes(x =1, y = 102, xend =30, yend = 102, color = \"red\")) +\n  geom_segment(aes(x = 1, y = 98.3, xend = 30, yend = 98.3, color = \"green\")) +\n  geom_segment (aes(x = 1, y = 97.2, xend = 30, yend = 97.2, color = \"blue\")) +\n  theme (legend.position = \"none\")\n\n\n\nSce2 |&gt;\n  group_by(Condition) |&gt;\n  summarise(mean = mean (DustYield))\n\n# A tibble: 3 × 2\n  Condition   mean\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Condition1 102. \n2 Condition2  98.3\n3 Condition3  97.2"
  },
  {
    "objectID": "week07.html#mfy-.unnumbered",
    "href": "week07.html#mfy-.unnumbered",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "8.6 MFY .{unnumbered}",
    "text": "8.6 MFY .{unnumbered}\n\nMFY &lt;- unicorns |&gt;\n  mutate (\"Y\" = DustYield) |&gt;\n  mutate (\"M\" = mean(Y)) |&gt;\n  # If we now ask R to group the data, it will calculate the mean per group:\n  group_by(Radio) |&gt;\n  mutate (\"F\" = mean(Y)) |&gt;\n  # Remember to ungroup after!\n  ungroup() \n\n\n\nMFY &lt;- MFY |&gt;\n  mutate (MY = (Y-M),\n          MF = (F-M),\n          FY = (Y - F))\n\n\nMFY &lt;- MFY |&gt;\n  mutate (MY2 = (MY*MY),\n          MF2 = (MF*MF),\n          FY2 = (FY*FY))\n\n\nMFY |&gt;\n  summarise(SumSquareMY = sum(MY2),\n            SumSquareMF = sum(MF2),\n            SumSquareFY = sum(FY2))\n\n# A tibble: 1 × 3\n  SumSquareMY SumSquareMF SumSquareFY\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      12053.       6301.       5752.\n\nMFY |&gt;\n  summarise(SumSquareMY = sum(MY2),\n            SumSquareMF = sum(MF2),\n            SumSquareFY = sum(FY2),\n            MeanSquareMY = sum(MY2)/29,\n            MeanSquareMF = sum(MF2)/2,\n            MeanSquareFY = sum(FY2)/27) \n\n# A tibble: 1 × 6\n  SumSquareMY SumSquareMF SumSquareFY MeanSquareMY MeanSquareMF MeanSquareFY\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1      12053.       6301.       5752.         416.        3151.         213.\n\nANOVA &lt;- aov(DustYield ~ Radio, data = unicorns)\nsummary(ANOVA)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nRadio        2   6301    3151   14.79 4.6e-05 ***\nResiduals   27   5752     213                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "week07.html#numerical-response",
    "href": "week07.html#numerical-response",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Numerical Response",
    "text": "Numerical Response\n\ncows |&gt; \n  summarise(mean_yield = mean(`Average Daily Yield`),\n            sd_yield = sd(`Average Daily Yield`))\n\n# A tibble: 1 × 2\n  mean_yield sd_yield\n       &lt;dbl&gt;    &lt;dbl&gt;\n1         32     3.73\n\nt.test(cows$`Average Daily Yield`, mu = 28)\n\n\n    One Sample t-test\n\ndata:  cows$`Average Daily Yield`\nt = 21.448, df = 399, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 28\n95 percent confidence interval:\n 31.63336 32.36664\nsample estimates:\nmean of x \n       32"
  },
  {
    "objectID": "week07.html#numerical-response-categorical-explanatory",
    "href": "week07.html#numerical-response-categorical-explanatory",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Numerical Response Categorical Explanatory",
    "text": "Numerical Response Categorical Explanatory\n\ndiet &lt;- tibble (before = c(5.04, 4.63, 4.04, 5.10, 5.43, 4.83, 3.45, 3.49, 5.02, 4.81),\n                after = c( 4.78, 2.49, 4.46, 2.03, 5.13, 7.23, 3.50, 1.89, 3.30, 3.91))\n\n\ndiet |&gt; \n  ggplot(aes(y = before)) +\n  geom_boxplot() +\n  geom_boxplot(aes(y = after, x =1)) +\n  theme_classic() +\n  scale_x_continuous(labels =c(\"Before Diet\", \"After Diet\"), breaks = c(0,1)) +\n  labs(x = \"Diet\", y = \"Weight (kg)\")\n\n\n\ndiet |&gt; \n  summarise(before_mean = mean(before),\n            after_mean = mean(after),\n            before_sd = sd(before),\n            after_sd = sd(after))\n\n# A tibble: 1 × 4\n  before_mean after_mean before_sd after_sd\n        &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1        4.58       3.87     0.689     1.62\n\nt.test(diet$before, diet$after, paired = TRUE, alternative = \"two.sided\")\n\n\n    Paired t-test\n\ndata:  diet$before and diet$after\nt = 1.4615, df = 9, p-value = 0.1779\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.3900511  1.8140511\nsample estimates:\nmean difference \n          0.712 \n\n\n2-Way ANOVA\n\nmodel1 &lt;-  aov(`Average Daily Yield` ~ parlour +  `housing type`, data = cows)\nsummary(model1)\n\n                Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nparlour          1    900   900.0  78.511 &lt; 2e-16 ***\n`housing type`   1    100   100.0   8.723 0.00333 ** \nResiduals      397   4551    11.5                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLinear model\n\nmodel2 &lt;- lm(`Average Daily Yield` ~ parlour +  `housing type`, data = cows)\nsummary(model2)\n\n\nCall:\nlm(formula = `Average Daily Yield` ~ parlour + `housing type`, \n    data = cows)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8651  -1.7814   0.0787   1.6805  11.7654 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      34.0000     0.2932 115.956  &lt; 2e-16 ***\nparlourrobot                     -3.0000     0.3386  -8.861  &lt; 2e-16 ***\n`housing type`seasonally housed  -1.0000     0.3386  -2.954  0.00333 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.386 on 397 degrees of freedom\nMultiple R-squared:  0.1801,    Adjusted R-squared:  0.176 \nF-statistic: 43.62 on 2 and 397 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "week07.html#numerical-response-numerical-explanatory",
    "href": "week07.html#numerical-response-numerical-explanatory",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Numerical Response Numerical Explanatory",
    "text": "Numerical Response Numerical Explanatory\nCorrelation\n\ncor.test(cows$`Average Daily Yield`, cows$`Welfare Score`)\n\n\n    Pearson's product-moment correlation\n\ndata:  cows$`Average Daily Yield` and cows$`Welfare Score`\nt = 1.7643, df = 398, p-value = 0.07845\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.01004724  0.18454894\nsample estimates:\n       cor \n0.08809126 \n\n\nLinear Model\nGenearlised Linear Model\n\n#| eval: true\n\n\nmodel4 &lt;- glm(`Average Daily Yield` ~ `Welfare Score` \n              + `housing type` + parlour, data = cows)\nsummary(model4)"
  },
  {
    "objectID": "week07.html#categorical-response",
    "href": "week07.html#categorical-response",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Categorical Response",
    "text": "Categorical Response\nProportion Test\n\nprop.test(x = 3, n = 20, p = 0.18, alternative = \"two.sided\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3 out of 20, null probability 0.18\nX-squared = 0.0033875, df = 1, p-value = 0.9536\nalternative hypothesis: true p is not equal to 0.18\n95 percent confidence interval:\n 0.03956627 0.38862512\nsample estimates:\n   p \n0.15"
  },
  {
    "objectID": "week07.html#categorical-response-categorical-explanatory",
    "href": "week07.html#categorical-response-categorical-explanatory",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Categorical Response Categorical Explanatory",
    "text": "Categorical Response Categorical Explanatory\nMcNemar’s Test\n\nwork &lt;- matrix(c(2, 5, 38, 35), \n               ncol=2, \n               byrow=TRUE,\n               dimnames = list(c(\"Dysplasia\", \"No Dysplasia\"),\n                               c(\"Before Work Season\", \"After Work Season\")))\n\n\nmcnemar.test(work)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  work\nMcNemar's chi-squared = 23.814, df = 1, p-value = 1.061e-06\n\n\nFisher’s Exact Test\n\nfisher.test(x = c(4,16), y = c(2,18))\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  c(4, 16) and c(2, 18)\np-value = 1\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.02564066        Inf\nsample estimates:\nodds ratio \n       Inf \n\n# Calculate Odds Ratio\n(4+18) / (16 + 2)\n\n[1] 1.222222\n\n\nChi2 Test\n\ngsd &lt;- matrix(c(16, 12, 84, 86), \n              ncol=2, \n              byrow=TRUE,\n              dimnames = list(c(\"Dysplasia\", \"No Dysplasia\"),\n                              c(\"Inbred\", \"Less Inbred\")))\n\n\nchisq.test(gsd)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  gsd\nX-squared = 0.30714, df = 1, p-value = 0.5794\n\nlibrary(vcd)\nassocstats(gsd)\n\n                     X^2 df P(&gt; X^2)\nLikelihood Ratio 0.57672  1  0.44760\nPearson          0.57481  1  0.44835\n\nPhi-Coefficient   : 0.054 \nContingency Coeff.: 0.054 \nCramer's V        : 0.054"
  },
  {
    "objectID": "week07.html#categorical-response-numerical-explanatory",
    "href": "week07.html#categorical-response-numerical-explanatory",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Categorical Response Numerical Explanatory",
    "text": "Categorical Response Numerical Explanatory\nLogistic Regression\n\ndysp &lt;- tibble(dysplasia = c(1, 1, 1, 1, 1, 1, 1,\n                             0, 0, 0, 0, 0, 0, 0,\n                             0, 0, 0, 0, 0, 0, 0),\n               inflammation =c(0.91, 0.79, 1.40, 0.71, 1.01, 0.77, 0.85,\n                               0.42, 1.02, 0.31, 0.05, 1.17, 0.04, 0.36, \n                               0.12, 0.02, 0.05, 0.42, 0.92, 0.72,  1.05)) \n\nlogit &lt;- glm(dysplasia ~ inflammation, data = dysp, family = \"binomial\")\nsummary(logit)\n\n\nCall:\nglm(formula = dysplasia ~ inflammation, family = \"binomial\", \n    data = dysp)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5715  -0.5727  -0.3094   1.0397   1.4914  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)    -3.190      1.484  -2.150   0.0316 *\ninflammation    3.488      1.740   2.004   0.0450 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 26.734  on 20  degrees of freedom\nResidual deviance: 20.534  on 19  degrees of freedom\nAIC: 24.534\n\nNumber of Fisher Scoring iterations: 5\n\nlibrary(easystats)\nreport(logit)\n\nWe fitted a logistic model (estimated using ML) to predict dysplasia with\ninflammation (formula: dysplasia ~ inflammation). The model's explanatory power\nis moderate (Tjur's R2 = 0.24). The model's intercept, corresponding to\ninflammation = 0, is at -3.19 (95% CI [-7.09, -0.88], p = 0.032). Within this\nmodel:\n\n  - The effect of inflammation is statistically significant and positive (beta =\n3.49, 95% CI [0.65, 7.91], p = 0.045; Std. beta = 1.47, 95% CI [0.27, 3.33])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\nparameters(logit)\n\nParameter    | Log-Odds |   SE |         95% CI |     z |     p\n---------------------------------------------------------------\n(Intercept)  |    -3.19 | 1.48 | [-7.09, -0.88] | -2.15 | 0.032\ninflammation |     3.49 | 1.74 | [ 0.65,  7.91] |  2.00 | 0.045\n\nexp(cbind(OddsRatio = coef(logit), confint(logit)))\n\n               OddsRatio        2.5 %       97.5 %\n(Intercept)   0.04118154 0.0008355444    0.4146414\ninflammation 32.71638289 1.9151146706 2735.6121035"
  },
  {
    "objectID": "week07.html#parametric-vs-non-parametric",
    "href": "week07.html#parametric-vs-non-parametric",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Parametric vs Non Parametric",
    "text": "Parametric vs Non Parametric"
  },
  {
    "objectID": "week07.html#power-calculations",
    "href": "week07.html#power-calculations",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Power Calculations",
    "text": "Power Calculations"
  },
  {
    "objectID": "week07.html#assumptions",
    "href": "week07.html#assumptions",
    "title": "8  Week 7: Analysing Quantitative Data",
    "section": "Assumptions",
    "text": "Assumptions\nResiduals\n\nresiduals &lt;- tibble(resids = resid(model4))\n\nresiduals |&gt; \n  ggplot(aes(x = resids)) +\n  geom_density()+\n  theme_classic()\n\n\nparameters::describe_distribution(residuals)"
  },
  {
    "objectID": "week089.html",
    "href": "week089.html",
    "title": "9  Weeks 8 & 9: Analytical Softwares",
    "section": "",
    "text": "This week has no content yet, please check back later!"
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "10  Week 10: Project Proposals",
    "section": "",
    "text": "This week has no content yet, please check back later!"
  },
  {
    "objectID": "refs.html",
    "href": "refs.html",
    "title": "11  References",
    "section": "",
    "text": "References\nThe cover image duck comes from Pixabay, as a Creative Commons 0 image by Clker-Free-Vector-Images-3736\n\n\nBrilleman, SL, MJ Crowther, M Moreno-Betancur, J Buros Novik, and R\nWolfe. 2018. “Joint Longitudinal and Time-to-Event Models via\nStan.” https://github.com/stan-dev/stancon_talks/.\n\n\nFellows, Ian. 2018. Wordcloud: Word Clouds. https://CRAN.R-project.org/package=wordcloud.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Brenton M.\nWiernik, and Dominique Makowski. 2022. “Easystats: Framework for\nEasy Statistical Modeling, Visualization, and Reporting.”\nCRAN. https://easystats.github.io/easystats/.\n\n\nPatil, Indrajeet. 2021. “Visualizations with\nstatistical details: The ’ggstatsplot’\napproach.” Journal of Open Source\nSoftware 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nStan Development Team. 2023. “RStan: The\nR Interface to Stan.” https://mc-stan.org/.\n\n\nTorchiano, Marco. 2020. Effsize: Efficient Effect Size\nComputation. https://doi.org/10.5281/zenodo.1480624.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007.\n“Residual-Based Shadings for Visualizing (Conditional)\nIndependence.” Journal of Computational and Graphical\nStatistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856."
  }
]